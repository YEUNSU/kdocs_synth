#!/usr/bin/env python3
"""
í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸
- train ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ë³„ ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¤‘ì¹˜ ê³„ì‚°
- ë°ì´í„° ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ìƒì„±
"""

import json
import torch
import pathlib
from typing import Dict, List
import argparse

def compute_class_weights(train_dir: str, output_file: str = "class_weights.json") -> Dict[str, float]:
    """í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    
    train_path = pathlib.Path(train_dir)
    
    if not train_path.exists():
        raise ValueError(f"train ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {train_dir}")
    
    # í´ë˜ìŠ¤ë³„ íŒŒì¼ ìˆ˜ ê³„ì‚°
    class_counts = {}
    class_names = []
    
    for class_dir in train_path.iterdir():
        if class_dir.is_dir():
            class_name = class_dir.name
            # dataset í´ë”ëŠ” ì œì™¸ (ë¶„í•  ê²°ê³¼ê°€ ì•„ë‹Œ ì‹¤ì œ í´ë˜ìŠ¤ë§Œ)
            if class_name == "dataset":
                continue
            file_count = len(list(class_dir.glob("*.jpg")))
            class_counts[class_name] = file_count
            class_names.append(class_name)
    
    if not class_counts:
        raise ValueError(f"train ë””ë ‰í† ë¦¬ì— í´ë˜ìŠ¤ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {train_dir}")
    
    # í´ë˜ìŠ¤ ì´ë¦„ ì •ë ¬
    class_names.sort()
    
    # íŒŒì¼ ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì •ë ¬ëœ ìˆœì„œ)
    frequencies = [class_counts[name] for name in class_names]
    
    print(f"=== í´ë˜ìŠ¤ë³„ íŒŒì¼ ìˆ˜ ===")
    for i, class_name in enumerate(class_names):
        print(f"  {class_name}: {frequencies[i]}ê°œ")
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¡œê·¸ ê¸°ë°˜)
    # log1p(x * 0.02)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì€ ê°’ì—ë„ ì ì ˆí•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    weights = (1.0 / torch.log1p(torch.tensor(frequencies, dtype=torch.float) * 0.02)).tolist()
    
    # ê°€ì¤‘ì¹˜ ì •ê·œí™” (ìµœëŒ€ê°’ì„ 1.0ìœ¼ë¡œ)
    max_weight = max(weights)
    normalized_weights = [w / max_weight for w in weights]
    
    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    weight_dict = dict(zip(class_names, normalized_weights))
    
    print(f"\n=== ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ===")
    for class_name in class_names:
        print(f"  {class_name}: {weight_dict[class_name]:.4f}")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(weight_dict, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ê°€ì¤‘ì¹˜ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
    
    return weight_dict

def analyze_class_distribution(train_dir: str):
    """í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    train_path = pathlib.Path(train_dir)
    
    if not train_path.exists():
        print(f"âŒ train ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {train_dir}")
        return
    
    # í´ë˜ìŠ¤ë³„ íŒŒì¼ ìˆ˜ ê³„ì‚°
    class_counts = {}
    total_files = 0
    
    for class_dir in train_path.iterdir():
        if class_dir.is_dir():
            class_name = class_dir.name
            # dataset í´ë”ëŠ” ì œì™¸ (ë¶„í•  ê²°ê³¼ê°€ ì•„ë‹Œ ì‹¤ì œ í´ë˜ìŠ¤ë§Œ)
            if class_name == "dataset":
                continue
            file_count = len(list(class_dir.glob("*.jpg")))
            class_counts[class_name] = file_count
            total_files += file_count
    
    if not class_counts:
        print(f"âŒ train ë””ë ‰í† ë¦¬ì— í´ë˜ìŠ¤ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {train_dir}")
        return
    
    print(f"=== í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ===")
    print(f"ì´ íŒŒì¼ ìˆ˜: {total_files}ê°œ")
    print(f"í´ë˜ìŠ¤ ìˆ˜: {len(class_counts)}ê°œ")
    
    # í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ê³„ì‚°
    class_names = sorted(class_counts.keys())
    
    print(f"\nğŸ“Š í´ë˜ìŠ¤ë³„ ë¶„í¬:")
    for class_name in class_names:
        count = class_counts[class_name]
        ratio = count / total_files * 100
        print(f"  {class_name}: {count:4}ê°œ ({ratio:5.1f}%)")
    
    # ë¶ˆê· í˜•ë„ ê³„ì‚°
    min_count = min(class_counts.values())
    max_count = max(class_counts.values())
    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
    
    print(f"\nğŸ“ˆ ë¶ˆê· í˜•ë„ ë¶„ì„:")
    print(f"  ìµœì†Œ í´ë˜ìŠ¤: {min_count}ê°œ")
    print(f"  ìµœëŒ€ í´ë˜ìŠ¤: {max_count}ê°œ")
    print(f"  ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1")
    
    if imbalance_ratio > 2.0:
        print(f"  âš ï¸  í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì‹¬í•©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
    else:
        print(f"  âœ… í´ë˜ìŠ¤ ë¶„í¬ê°€ ë¹„êµì  ê· í˜•ì ì…ë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--train_dir", "-t", default="dataset/train", help="train ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--output", "-o", default="class_weights.json", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("--analyze", "-a", action="store_true", help="ë¶„í¬ ë¶„ì„ë§Œ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    try:
        if args.analyze:
            # ë¶„í¬ ë¶„ì„ë§Œ ì‹¤í–‰
            analyze_class_distribution(args.train_dir)
        else:
            # ê°€ì¤‘ì¹˜ ê³„ì‚° ë° ì €ì¥
            weights = compute_class_weights(args.train_dir, args.output)
            
            # ì¶”ê°€ ë¶„ì„
            print(f"\n" + "="*50)
            analyze_class_distribution(args.train_dir)
            
            print(f"\nâœ… ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ!")
            print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {args.output}")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 